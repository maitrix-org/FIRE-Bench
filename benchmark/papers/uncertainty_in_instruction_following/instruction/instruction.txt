You are a research agent. Conduct research and experiment about the question: "How effectively can large language models (LLMs) estimate their uncertainty when following user instructions, and how can this ability be systematically evaluated in a controlled setting?"

You have access to the following resources:

Models:
- OpenAI's gpt-3.5-turbo
- Load with HuggingFace: meta-llama/Llama-2-7b-chat
- Load with HuggingFace: meta-llama/Llama-2-13b-chat
- Load with HuggingFace: mistralai/Mistral-7B-Instruct-v0.3

- You can call these models using: from utils.llm_inference import LLMInference
- API key is provided with the LLMInference initialization function
- You can use the batch_generate() function to speed up the experiment
- Computational budget: 10000 API calls per model

Datasets:
- Two distinct versions of the dataset: Controlled and Realistic: /data/controlled_ver.jsonl and /data/realistic_ver.jsonl
- Introduce two levels of controlled difficulty–Controlled-Easy and Controlled-Hard by generating three categories of responses: completely incorrect, correct, and subtly off-target
- In the Controlled-Easy, calculate AUROC based on distinguishing between correct and completely incorrect responses
- In the Controlled-Hard, calculate AUROC based on distinguishing between correct and subtly off target responses.

Evaluation Methods:
- Verbalized confidence: The model’s self-reported confidence, scored from 0 to 9, indicating its perceived likelihood that the response correctly follows instructions.
- Normalized p(true) and p(true): These methods assess the probability of the ‘true’ token, calculated from a binary choice prompt.
- Perplexity and Sequence probability: Perplexity measures the likelihood of generating a given sequence.
- Mean token entropy for LLMs: Entropy measures uncertainty based on token prediction variability.

Experimental constraints:
- You should **NOT** use any web search tool or try to answer the question based on your prior knowledge, but run **FULL** end to end experiment and draw conclusion from your experiment results. 
- When loading dataset using ```load_dataset("google/IFEval")```, don't try to add other arguments like "cache_dir".


Please design and execute experiments to investigate this research question. Document your experimental plan, run end-to-end experiments, and provide conclusions.