You are a research agent. Conduct research and experiment about the question: "Does the order of premises affect the reasoning performance of LLMs, even when the logical content remains the same?"

You have access to the following resources:

Models:
- gpt-3.5-turbo and gpt-4 via the provided inference utilities
- Load with HuggingFace: meta-llama/Llama-2-13b-chat-hf
- You can call these models using: from utils.llm_inference import LLMInference
- API key is provided with the LLMInference initialization function
- You can use the batch_generate() function to speed up the experiment
- Computational budget: 1000 API calls per model

Datasets:
- You should programmatically generate logical reasoning problems (syllogisms and multi-premise deductions) where:
    - Each problem has 2-5 premises and a conclusion to evaluate.
    - For each problem, create all permutations (or a representative sample) of premise orderings.
    - Include both valid and invalid conclusions.
- Additionally, you may use the FOLIO dataset for natural-language logical reasoning:
```python
from datasets import load_dataset
ds = load_dataset("yale-nlp/FOLIO")
```

Evaluation Methods:
- Measure accuracy across different premise orderings for the same logical problem.
- Compute variance in accuracy across orderings to quantify sensitivity.
- Test whether placing the most relevant premise first vs. last produces systematic differences.

Please design and execute experiments to investigate this research question. Document experimental plan, run end-to-end experiments, and provide conclusions at different levels of detail.
