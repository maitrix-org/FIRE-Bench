You are a research agent. Conduct research and experiment about the question: "Are large language models metacognitively aware of when they are hallucinating or producing unreliable outputs?"

You have access to the following resources:

Models:
- o1-like models: o1, o1-mini
- Conventional models: GPT-4o-mini, GPT-4o
- Load with HuggingFace: meta-llama/Llama-3.1-8B
- You can call these models using: from utils.llm_inference import LLMInference
- API key is provided with the LLMInference initialization function
- You can use the batch_generate() function to speed up the experiment
- Computational budget: 1000 API calls per model

Datasets:
- You should construct a mix of questions where:
    - Some have well-known, verifiable answers (the model is expected to answer correctly).
    - Some are obscure, recently emerged, or fictional (the model is likely to hallucinate).
    - Some are trick questions or contain false presuppositions.
- For factual questions, you may use:
```python
from datasets import load_dataset
ds = load_dataset("trivia_qa", "unfiltered")
```

Experimental Design:
- For each question, first obtain the model's answer, then ask the model to rate its own confidence that the answer is correct (e.g., on a 0-100 scale).
- Separately verify the factual correctness of each answer.
- Analyze the relationship between self-reported confidence and actual correctness.

Evaluation Methods:
- Compute calibration metrics: Expected Calibration Error (ECE), Brier score, AUROC for separating correct vs. incorrect answers using self-reported confidence.
- Measure whether models assign lower confidence to hallucinated answers versus correct ones.
- Test whether explicitly prompting the model to consider whether it might be wrong improves calibration.

Please design and execute experiments to investigate this research question. Document experimental plan, run end-to-end experiments, and provide conclusions at different levels of detail.
